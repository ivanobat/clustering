---
title: "Clustering analysis of product features"
date: "2021-11-19"
author: "Ivan Aguilar"
output: 
  html_document:
    toc: true
    theme: united
---

Pratice:

This is a real business dataset from a large e-commerce company. The company has a global and diverse infrastructure which causes many identical products to be classified differently. To conduct sound and insightful product analysis, the company needs to be able to accurately cluster similar products. The dataset has 93 features for 10,000 products and your objective is to cluster the products in coherent groups. You will implement different clustering algorithms, at least one hard clustering and one soft clustering and compare your results. You are working in an unsupervised setting so you do not know the true number of groups and will have to use different statistics to pick an optimal number of groups.

Optional: on the day of the deadline, the true labels / classes will be released and you will be able to compare your clustering to the ground truth.

Submit your analysis and code in R markdown format, hand in both the .Rmd file and the knitted Rmd. Please submit one zip file on Classroom that gathers all the files you submit.

```{r setup, messages=FALSE}
# Reset environment variables
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```


# Loading libraries and data initial exploration

First of all we load the libraries we will be using and the dataset including the product features from the csv file

```{r include=FALSE}

#Load libraries
library(readr)
library(dplyr)
library(cluster) #clustering
library(factoextra) # clustering algorithms & visualization
library(gridExtra) # multiple plotting with ggplot2
library(mclust) # clustering with mixture model
library(dbscan) #soft clustering option

#Load data
PATH= './clustering'
product_features <- read_csv(file.path(PATH,"product_features.csv"));

```

Display a small preview of the loaded data. 

```{r}
head(product_features)
```

At first sight the dataset looks very sparse with a big amount of zeros for each row, let's have a general look

```{r}
summary(product_features)
```

The initial intuition is correct, data is very sparse but also there are quite relevant outliers on each feature. 

Let's count now the percentage of zero entries for each column(feature), so that will give us a measurement of sparcity:

```{r}
res <- colSums(product_features==0)/nrow(product_features)*100
res
sum(res>=75)
```

The above tells us that 67 out of 94 features have 75% or more rows with zero entries. This in itself will condition the usage of certain methods of clustering later.

# Data preparation

Next we create a new dataframe were we will apply some clean up tasks:

- Remove the id column and make it a row index
- Drop any missing data
- Scale the whole dataset

NOTE: The complete dataset of 10000 rows takes a long time to process so we will random sample the dataset to only 1000 observations. The conclusions derived are not significantly affected by this observation reduction.

```{r}
rows = 1000
set.seed(11) # set a seed for reproducing results on the subset random sampling
df <- sample_n(product_features ,size=rows)[,-1]   # move the data to dataframe removing the first column and random sampling
df <- na.omit(df)  # drop missing values
df <- scale(df)    # rescale the data
rownames(df)= product_features[1:rows,]$id # Set the product id as row index of the dataframe
head(df) # preview the dataframe we will be working with
```
# Hard clustering with Kmeans and PAM (Partitioning around mediods) built-in functions

Let's explore some hard clustering techniques using built-in kmeans and PAM functions, recalling hard clustering refers to when an object is assigned to 1 and only 1 cluster.

### Graphical exploration
First we will graphically explore what would be some of the clustering setups by analyzing the convex hulls in a 2D graph provided by ecluster library.

To define the clusters we will use two methods: K-means and PAM and euclidean distances for both. The main difference between Kmeans and PAM relies on the fact that Kmeans uses artificially generated centroids to initialize cluster positions while PAM uses already existing points to set them as centroids (medodoids).

We will arbitrarily plot these two methods clustering with a predefined number of clusters = 3. Later on we will analyze which is the best k on each case.

### Kmeans using eclust

We will use the function eclust to cluster and graph the results of the product features clustering. Eclust calls upon built-in functions with its FUNcluster argument and then also graphs the results (similar to fviz_cluster). All arguments passed natively to clustering methods like Number of clusters or metric can also be specified on this function.

Let's have a look at how a kmeans, 3-cluster, euclidean setup would look like:
```{r}
km0 <- eclust(df,FUNcluster="kmeans", k=3, hc_metric = "euclidean")
```

It is also possible to recover the cluster assignment for each feature from the resulting object of eclust, here below we look at the first 20 observation cluster assignments
```{r}
head(km0$cluster,20)
```

### PAM using eclust
As we mentioned earlier, PAM clustering method varies slightly how the centroids are determined, so we will also run the clustering with similar arguments for this method and analyze the differences.

```{r}
pm0 <- eclust(df,FUNcluster="pam", k=3, hc_metric = "euclidean")
```

Looking at the first 20 cluster assignments for the PAM method

```{r}
head(pm0$cluster,20)
```

Evidently we can already appreciate some differences on the clustering by different methods for this k=3 example. But on this 2-d representation will not let us drive further conclusion other than speculative ones based on shape.

Now looking at the data itself we can also reflect that difference, but can't dig further into it as the cluster assignment numbers are not comparable 1 to 1, but just out of curiosity we display them together below:

```{r}
head(km0$cluster,20)
head(pm0$cluster,20)
```

### Comparing kmeans and PAM with multiple k settings
To continue our graphical exploration of the clustering methods we now map a few more k values to appreciate the difference with different number of clusters, again for both kmeans and pam methods

NOTE: for kmeans we have overriden the deafult iter.max argument, because 10 iterations was not enough to converge

```{r}
k2 <- kmeans(df, centers = 2, nstart = 5, iter.max=20)
k3 <- kmeans(df, centers = 3, nstart = 5, iter.max=20)
k4 <- kmeans(df, centers = 4, nstart = 5, iter.max=20)
k5 <- kmeans(df, centers = 5, nstart = 5, iter.max=20)

p2 <- pam(df, k = 2, nstart = 5)
p3 <- pam(df, k = 3, nstart = 5)
p4 <- pam(df, k = 4, nstart = 5)
p5 <- pam(df, k = 5, nstart = 5)
```

We plot kmeans clustering results using fviz_cluster function for k=2,3,4,5

```{r}
# plots to compare
pk1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
pk2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
pk3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
pk4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

grid.arrange(pk1, pk2, pk3, pk4, nrow = 2)
```
Similarly for PAM clustering using fviz_cluster for k=2,3,4,5

```{r}
# plots to compare
pp1 <- fviz_cluster(p2, geom = "point", data = df) + ggtitle("k = 2")
pp2 <- fviz_cluster(p3, geom = "point",  data = df) + ggtitle("k = 3")
pp3 <- fviz_cluster(p4, geom = "point",  data = df) + ggtitle("k = 4")
pp4 <- fviz_cluster(p5, geom = "point",  data = df) + ggtitle("k = 5")

grid.arrange(pp1, pp2, pp3, pp4, nrow = 2)
```

### What is the best K using KMeans and PAM
After doing some basic visual analysis let's now use some formal metrics to determine what is the optimal k to select. For this purpose we will continue to use Kmeans and PAM for determining the clusters and we will test the best selection with three metrics:

1. Within cluster distance (wss)
2. Gap-statistic
3. Silhouette

We will start first with kmeans and wss and determine what is our best K based on the elbo graph.

For graphing these results we will use function fviz_nbclust which determines and visualizes the optimal number of clusters by using the function and  method we specify on the arguments FUNcluster and method respectively. The k.max argument determines for how many clusters the function will be run for, the default is 10 but we will increase it to 15. 

```{r}
fviz_nbclust(df, kmeans, method="wss", k.max=10)+theme_classic()
```

The above graph suggest the the optimal number of cluster is 2 (slightly)

Using the same approach but now with PAM:
```{r}
fviz_nbclust(df, pam, method="wss", k.max=10)+theme_classic()
```

PAM elbo suggests that the optimal number of clusters is 3. An small discrepancy between clustering functions but that also could be interpreted differently as the elbo for kmeans also shows some curve in k=2

Now let's use the gap-statistic strategy, first using kmeans:

NOTE: for the gap statistic we will not use fviz_nbclust as it doesn't allow us to tune the iter.max parameter directly and if left with default value 10, some of the iterations do not converge

```{r}
km_gap_stat <- clusGap(df, FUN = kmeans, nstart = 5, K.max = 10, B = 5, iter.max=20) 
fviz_gap_stat(km_gap_stat)
#fviz_nbclust(df, FUNcluster=kmeans, method="gap_stat", k.max=15)+theme_classic()
```

The kmeans and gap-statistic combination suggest that the optimal number of clusters is 7

We try the gap statistic method but now with PAM function:
```{r chunk-forever}
pm_gap_stat <- clusGap(df, FUN = pam, nstart = 5, K.max = 10, B = 5) 
fviz_gap_stat(pm_gap_stat)
#fviz_nbclust(df, FUNcluster=pam, method="gap_stat", k.max=10)+theme_classic()
```

Using PAM, the gap-statistic metric suggest the optimal number of clusters is 10. But it is probable that increasing the k.max parameter will also change the election for best number of clusters on this case.

# Soft clustering
In soft-clustering, all data points exist in all the clusters with some probability, so at the same time data points belong to multiple clusters opposing the hard clustering approach.

For the soft-clustering part of this exercise we will use the built-in mclust package. 

For this, let's first show the different types of Gaussian mixture models (GMM) that mclust considers, so we can refer to it later.

```{r, echo=FALSE, out.width = '50%'}
knitr::include_graphics(file.path(PATH,"types_gmm_mclust.PNG"))
```

### Soft clustering with mclust
Let's also remember that mclust uses BIC to determine which is the best model, which will be given by a combination of the type of GMM and number of components. Let's run the function on our product features dataset for up to 10 clusters

```{r chunk-forever_again}
mod <- Mclust(df, G=1:10)
```

And look at the general summary of the results which includes the clustering output and wining model
```{r}
summary(mod)
```

We can see the function suggest a VEI GMM with 10 components. Compared to our hard clustering results, this result comes close to the pam/gap-statistic result, but sets itself quite apart from the other methods we tried during hard clustering like elbo.

Let's look at more detail at the BIC results, where we can see the top three models are:

VEI, 10
VEI, 9
VEI, 8

The VEI GMM dominates with BIC from 2 clusters onward and that trend continues as the number of clusters increases. 

```{r}
summary(mod$BIC)
```
Just to confirm the statement above let's plot the BIC values for each type of GMM:

```{r}
plot(mod, what = "BIC", ylim = range(mod$BIC[,-(1:2)], na.rm = TRUE),
     legendArgs = list(x = "bottomleft"))
```

The VEI GMM dominates from 3 clusters onward but the BIC metric will continue to improve as we add more clusters when running the mclust function, however it is also visible that some cluster contain very few members vs the total

### Soft clustering with dbscan

We will also attempt soft clustering with dbscan. Dbscan is good for data for which hierarchical methods don't work too well due to noise and outliers. The algorithm of dbscan groups the clusters according a defined distance, typically euclidean, but there are a couple of parameters that need to be set too:

1. Minpts: determines what is the minimum of points a cluster can contain
2. eps: This is the distance that will be used as radium of the clusters

The eps paremeter can be choosen by running a kdistance plot

```{r}
kNNdistplot(df, k = 3 )
```

An appropriate value for the eps would be around 5. 

```{r}
Dbscan_cl <- dbscan(df, eps=5.5, minPts=3)
Dbscan_cl
```

Dbscan suggest 6 clusters, but most of them with the minimal number of points (3), let's have a look at what the assignment looks like:

```{r}
# Checking cluster
Dbscan_cl$cluster
```
and if we plot those cluster's convex hulls in 2d we see the following:

```{r}
# Plotting Cluster
hullplot(df, Dbscan_cl$cluster,  main = "Convex cluster Hulls")
```

In general dbscan doesn't seem to be a good function for this dataset as it's radio distance method is unable to find a lot of close data points to form relevant clusters and more over, leaves a lot of points unnassigned.

### Comparison hard vs soft clustering

The soft clustering methods have all suggested a lower number of clusters, except for the ones using the gap-statistic method that suggested 7 and 10 clusters which resembles the 10 suggested by mclust on soft clustering. Dbscan was not able to produce a significant result, although it suggested 5 cluster

With soft clustering once a good GGM is found the method keeps on improving as additional clusters are assigned, so in general the clusters seemed to be better defined and consistent.

Finally comparing the first 20 products between the pam hard clustering and the mclust soft clustering assignment, there is not much similarities, pattern-wise, i.e. the assignments seem to differ without any evident pattern regardless of having the same number of clusters, but that is expected.

```{r}
pm9 <- pam(df, k=9, nstart=5)

head(pm9$cluster,20)
head(mod$classification,20)
head(Dbscan_cl$cluster,20)
```

### Soft clustering projection in 2d for mclust

Another way to visualize our soft clustering with GMMs is to use mclustdr package which will allow us to visualize the clustering structure and geometric characteristics of our winning model by project the data onto a suitable dimension reduction subspace.
(full details here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/)

First we use our GMM model results and fit it with drmod

```{r}
drmod <- MclustDR(mod, lambda = 1)
```

And then we plot the results:
```{r}
plot(drmod, what = 'contour')
```

We can see the areas where the clusters interact with each other and how some points could belong to different clusters overlap.

Finally, from the function mclustdr we can also obtain a summary which shows the estimated directions, which span the reduced subspace, as a set of linear combinations of the original features, ordered by importance as quantified by the associated eigenvalues

```{r}
summary(drmod)
```

